to use mongo shell in docker: docker exec -it mongodb-database /bin/sh 
-- mongodb-database is the container name

- either don't have any environment variables in docker compose file if we don't want to make it secure
- else use command: mongo admin -u siddhant -p 'dbpass' 
- or use command: 

        mongo admin -u admin -p 'pass' --authenticationDatabase admin


- where uname and password should match what is given in docker compose file
- same can be checked using 'env' command since they're environment variables

mongodb is no sql database 
- non relational  database
- document based

type of nosql database:
    - document based
    - key-value based
    - wide-column database
    - graph database

- type 'mongo' in unix shell to open mongo shell and exit() to exit mongo shell
- 'show dbs' command in mongo shell to see list of databases and space they have occupied
    -- if we create any new database it will not show in list of databases (by showdbs) 
        until it has atleast 1 document
- use siddhantdb -- to create a new database and start using it
- db -- to check which database is being currently used
- db.createCollection('FacebookComments') -- to create a new collection in currentl database
- db.instagramComments.insert({username:'siddhant_jain5', post_id:'instagram.com/ab4df', comment:'good'})
    -- 2nd way of creating table <- we directly insert a record in the table and if table is not present,
        it will create the table and insert the record
-- always prefer first way (createCollection command), we can provide additional paramerters in that
- db.createCollection(collection_name, {capped:true, size:5242880, max:5})
    -- capped true means that this collection can have maximum of 5 documents or less
    -- and maximum size cannot exceed 5242880 
    -- if it reaches in 4 documents then only 4 documents will be there
    -- if it reaches in 6 documents then 5 documents will be there

mongodb has two type of documents
- relational documents
- embedded documents (preferred)(better for scalability)

- db.student.insert({regNo: "1234",name: "student1", course: {courseName: "MCA", duration: "3 years"},address: { city: "bangalore", state: "KA", country: "India"}})
-- here couse and address are embedded documents
-- both of them do not have any primary key of their own and are dependent on student object

- mongodb create a unique objectId for each document which is nothing but a primary key (key="_id)
- we can override and mention our own primary key by providing value to _id key
- db.student.insert({_id: "5678", regNo: "5678",name: "student2", course: {courseName: "MCA", duration: "3 years"},address: { city: "bangalore", state: "KA", country: "India"}})
- _id is the only key which is used as primary key, we can't provide any other 

-- to do bulk insert write following in mongo shell
var bulkDocuments = [{username:'siddhant_jain5', post_id:'instagram.com/ab4df', comment:'good'}, {username:'insta_user', post_id:'instagram.com/avg75v', comment:'better'}, {username:'fb_user', post_id:'instagram.com/kjnrke', comment:'best'}]
db.FacebookComments.insert(bulkDocuments)
var bulkDocuments2 = [{username:'siddhant_jain6', post_id:'instagram.com/ab4df', comment:'good'}, {username:'quora_user', post_id:'instagram.com/avg75v', comment:'better'}, {username:'linkedin_user', post_id:'instagram.com/kjnrke', comment:'best'}]
db.FacebookComments.insert(bulkDocuments2)

-- update records in mongodb
- db.FacebookComments.update({comment:"good"},{$set:{"comment":"more than good"}})
-- when we use update, it only changes 1 record even if multiple documents satisfy the condition
-- it starts from top, updates the first document with matching condition and the stops

- db.FacebookComments.update({comment:"better"},{$set:{"comment":"all better"}}, {multi:true})
-- {multi:true} will change all records matching the condition

-- upsert in mongodb
- db.FacebookComments.update({comment:"better"},{$set:{"comment":"all better"}}, {upsert:true})
-- if it finds a record with comment "better" it updates that record
-- else it create a new record with just one key:value i.e. what we mentioned in set command

-- delete document in mongodb
- db.FacebookComments.remove({"comment":"all better"})
-- remove command will remove all records (unlike update command)

- show collections -- to get list of collections in current database
- db.instagramComments.find() -- to list all records in a collection
- db.instagramComments.find().pretty() -- to format documents properly like a json

-- to query documents based on some condition
- db.FacebookComments.find({comment:"best"})
-- it will return all the records which match the condition
-- if we give any key which is not in documents then it will give empty result and not error
    bcz it is nosql so no fixed schema, key can be present in one doc and not present in another

db.FacebookComments.find().limit(2)
-- to limit number of records displayed (first 2 records will be shown)

--sort the records
db.FacebookComments.find().sort({username: -1})
-- -1 will sort it in descending order
-- 1 will sort in ascending order

-- search based on embedded column
db.student.find({"address.city": "bangalore"})

-- find all students with student ids between a range
- db.student.insert({regNo: "1240",name: "student3", course: {courseName: "MCA", duration: "3 years"},address: { city: "jhansi", state: "UP", country: "India"}})
- db.student.insert({regNo: "1284",name: "student4", course: {courseName: "MCA", duration: "3 years"},address: { city: "jhansi", state: "UP", country: "India"}})
- db.student.insert({regNo: "1354",name: "student5", course: {courseName: "MCA", duration: "3 years"},address: { city: "jhansi", state: "UP", country: "India"}})
- db.student.insert({regNo: "5234",name: "student6", course: {courseName: "MCA", duration: "3 years"},address: { city: "jhansi", state: "UP", country: "India"}})

- db.student.find({regNo: {$gt: "5000"}}) -- get all record will regNo greater than(gt) 5000 
    -- since input is a string if we provide 5000 without quotes in query, it will not return anything
- db.student.find({regNo: {$lt: "5000"}})
- db.student.find({regNo: {$in: ["1240","1340"]}}) -- only records which have regNo as either 1240 or 1340
- db.student.find({regNo: {$gt: "1240", $lt:"1354"}}) -- boundaries exclusive (1240 and 1354 will not be in o/p)

-- indexing in mongodb
- db.student_new.insert({regNo: "1234",name: "student1", section:"F4", course: {courseName: "MCA", duration: "3 years"},address: { city: "bangalore", state: "KA", country: "India"}})
- db.student_new.insert({regNo: "5678",name: "student2", section:"F2", course: {courseName: "MCA", duration: "3 years"},address: { city: "bangalore", state: "KA", country: "India"}})
- db.student_new.insert({regNo: "1240",name: "student3", section:"F4", course: {courseName: "MCA", duration: "3 years"},address: { city: "jhansi", state: "UP", country: "India"}})
- db.student_new.insert({regNo: "1284",name: "student4", section:"F2", course: {courseName: "MCA", duration: "3 years"},address: { city: "jhansi", state: "UP", country: "India"}})
- db.student_new.insert({regNo: "1354",name: "student5", section:"F4", course: {courseName: "MCA", duration: "3 years"},address: { city: "jaipur", state: "UP", country: "India"}})
- db.student_new.insert({regNo: "5234",name: "student6", section:"F2", course: {courseName: "MCA", duration: "3 years"},address: { city: "jaipur", state: "UP", country: "India"}})
- db.student_new.explain("executionStats").find({section: "F4"})
-- under executionStats there is a key called totalDocsExamined: its value is 6 
    -- which is total number of records in the collection right now
-- if there are millions of records, then it will go through all million records to search for given criteria

- db.student_new.createIndex({section: 1})
 -- to apply index on section key,and 1 to have it in ascending order
 -- this will add another index, and not remove current indexes
 -- since before it was 1, if we run getIndexes now it will give 2
 -- now executionStats command will show totalDocsExamined value as 3 
    -- which is no. of rec having this value

- db.student_new.dropIndex("section_1")
 -- if we check in getIndexes this will be mentioned as name
 -- first is the key name followed by underscore followed by ascending or descending
 -- if we just say key name it will throw error saying index not found

-- aggregations in mongodb
$project -> to select specific fields
$match -> filter operation
$group -> aggregation operation
$sort -> sort operation
- db.student_new.aggregate([{"$match": {$and: [{"section":"F4"}, {"regNo":{"$gt":"1239"}}]} }])
- db.student_new.aggregate([{"$project":{"name":1, "section":1, "address.city":1}}])
    -- if we mention 1 it will be displayed and if we mention 0 then not displayed
    -- we can also skip the columns we don't want to select
    -- for _id column we explicitly need to mention 0 otherwise it will be displayed

- db.student_new.aggregate([{"$project":{"name":1, "section":1, "address.city":1, "_id":0}}])
- db.student_new.aggregate([{"$match": {"section":"F4"}}, {"$project":{"name":1, "section":1, "address.city":1, "_id":0}}])

-- group by aggregations
- db.student_marks.insert({regNo: 1234,name: "student1", section:"F4", marks:80.5, course: {courseName: "MCA", duration: "3 years"},address: { city: "BLORE", state: "KA", country: "India"}})
- db.student_marks.insert({regNo: 5678,name: "student2", section:"F2", marks:70.5, course: {courseName: "MCA", duration: "3 years"},address: { city: "BLORE", state: "KA", country: "India"}})
- db.student_marks.insert({regNo: 1240,name: "student3", section:"F4", marks:65, course: {courseName: "MCA", duration: "3 years"},address: { city: "jhansi", state: "UP", country: "India"}})
- db.student_marks.insert({regNo: 1284,name: "student4", section:"F2", marks:92.5, course: {courseName: "MCA", duration: "3 years"},address: { city: "jhansi", state: "UP", country: "India"}})
- db.student_marks.insert({regNo: 1354,name: "student5", section:"F4", marks:41.2, course: {courseName: "MCA", duration: "3 years"},address: { city: "jaipur", state: "RAJ", country: "India"}})
- db.student_marks.insert({regNo: 5234,name: "student6", section:"F2", marks:33, course: {courseName: "MCA", duration: "3 years"},address: { city: "jaipur", state: "RAJ", country: "India"}})
- db.student_marks.insert({regNo: 1235,name: "student7", section:"F4", marks:35, course: {courseName: "MCA", duration: "3 years"},address: { city: "BLORE", state: "KA", country: "India"}})
- db.student_marks.insert({regNo: 5676,name: "student8", section:"F2", marks:95, course: {courseName: "MCA", duration: "3 years"},address: { city: "BLORE", state: "KA", country: "India"}})
- db.student_marks.insert({regNo: 1236,name: "student9", section:"F4", marks:48, course: {courseName: "MCA", duration: "3 years"},address: { city: "jhansi", state: "UP", country: "India"}})
- db.student_marks.insert({regNo: 1237,name: "student10", section:"F2", marks:12, course: {courseName: "MCA", duration: "3 years"},address: { city: "jhansi", state: "UP", country: "India"}})
- db.student_marks.insert({regNo: 1338,name: "student11", section:"F4", marks:48, course: {courseName: "MCA", duration: "3 years"},address: { city: "jaipur", state: "RAJ", country: "India"}})
- db.student_marks.insert({regNo: 5239,name: "student12", section:"F2", marks:100, course: {courseName: "MCA", duration: "3 years"},address: { city: "jaipur", state: "RAJ", country: "India"}})

- db.student_marks.aggregate([{"$group":{"_id": {"section": "$section"}, "TotalMarks":{"$sum": "$marks"}}}])
    -- id is compulsory in group by
- db.student_marks.aggregate([{"$group":{"_id": {"section": "$section"}, "TotalMarks":{"$sum": "$marks"}, "Count":{$sum:1}, "AverageMarks":{$avg: "$marks"}}}])
- db.student_marks.aggregate([{$match:{section: 'F4'}},{"$group":{"_id": {"section": "$section"}, "TotalMarks":{"$sum": "$marks"}}}])

- db.student_new.getIndexes() --> go get which key is current index, by default '_id'
- db.student.count() --find number of documents in a collection

- all mongodb database are stored in /data/db folder
- table in sql is collection in mongodb
- row in sql is document in mongodb (single record)

-- replication in mongodb
-- replication and high availability are main features of mongodb
-- they come inbuilt with mongodb and no separate s/w installation is required
-- replication done with master(one)-slave(many) architecture
-- can me done using mongodb atlas
-- in atlas we can create one free cluster (which is by default 3 node cluster)
-- one primary node and two secondary
-- we can insert record only in primary node and read from secondary also
-- we need to provide network access and database access in atlas

--------------------------------------------------------------------
-- mongo db stores bson (which are binary json)
-- max document size in mongodb can be 16mb
-- mongodb doesn't enforce any foreign key relationship
    -- there can be soft foreign keu relation ship
    -- like book collection can have publisherid key
    -- and there can be a separate publisher collection where we can get all info 
        -- of a publisher using the publisherid
    -- but no rules like publisherkey should be mandtorily be present in
        -- publisher collection are enforced by mongodb (we need to take care of that)
-- any table which is a dependent of another table can be an embedded document
    -- for eg. rating is dependent on books or address is dependent on customers
    -- these documents(rating/address) have no meaning by themselves
    -- these do not need to be separate collection in mongodb
    -- they can be a document or array of documents inside the collection they're dependent on
    -- this saves us from performing several joins to get this data
    -- drawback is it's a bit of overhead if we mostly don;t require ratings info with books
-- when to use embedded documents
    -- check if embedded doc is wanted 80% time or not?
        -- for eg. if we are displaying books on e-commerce website where we displayed
            ratings everytime, then we should keep it as an embedded document
        -- suppose on a page the user only sees the list books that he has purchased
            and doesn't require ratings since it is already purchased. then no a good
            idea to go with embedded documents.
    -- is embedded data a bounded set?
        -- for eg rating will keep growing for a book as the site and the book 
            gain popularity. so as more people rate the document will keep getting bigger 
            and bigger and hence will not be a good idea to embed it
            it might surpass 16mb size at some point which is not possible in mongodb
        -- but if that embedded doc size if limited than fine, like a customer might have 
            max of 2-3 addresses
    -- how varied are your queries?
        -- for eg. if a query is normal like get all list of books with its ratings, then 
            embedded documents is a good idea
        -- but if they're different type of queries, then data modelling should differ
    -- whether it's integration db or application db
        -- application db is which we use for a small scale application. where each application
            has a single db associated to it.
        -- integration db are used by large enterprise, where lot of different application
            use the same db
        -- all application should agree with the defined data model
        -- easier to design data for application db

- video course on mongodb and ODM (object document mapper) similar to ORM
    https://www.youtube.com/watch?v=E-1xI85Zog8
- Udemy course of the exact same vide (broken into lesson)(enrolled in udemy account)
    https://www.udemy.com/course/draft/1358396/learn/lecture/8222114?start=0#overview
- github repo of the above video:
    https://github.com/mikeckennedy/mongodb-quickstart-course/tree/master/src/snake_bnb/src

check for $elemmatch $gte $lte 

-----------------------------------------------------------------------------------------

##### from udemy course #####

mongod is the database server, it should always be running if we want to connect to it
by default it runs on port 27017, we can change it by adding --port to mongod command
same --port can be used with mongo command to open shell and connect it to server
in windows it runs as a service in background and we don't need to manually run it
but in linux and mac we need start it manually and keep it running as long as we want to access it
the command for it is: mongod
it looks for /db/data path in root folder or c/users_username (windows), 
    if path is not present it fails to start
if we want to store db in a different path then command is: mongod --dbpath "path of /db/data"

difference b/w update and (updateOne or updateMany):
    -- in updateOne or updateMany it is compulsory to pass a operator,
    -- if we try without any operator for eg. $set, it will throw error
    -- but with update it will simply create a new document with field passed
        and relace it with existing document (objectId of all the replaceed documents will be same) 
    -- for eg there are two documents {_id:1, name: random1, salary: 5},{_id:2, name: random2, salary: 10}
        and we write update query like db.emp.update({name:random2}, {age15})
        expectation is {_id:1, name: random1, salary: 5, age:15}, but it will become {_id=1, age=15}
        correct way of doing was db.emp.update({name:random2}, {$set: {age15}})
    -- so better to use updateOne and updateMany
    -- simple update command can work like replace, but there is different func for replace also

find doesn't return a array of documents as it looks like
instead it sends a cursor object (which has lot of metadata)
bcz data can be huge so sending all at once can be very costly
to get as array
- db.student_new.find().toArray()
.pretty() is a cursor function as hence will not work with commands which return
a document or array of documents (eg. findOne())


data types in mongodb:
- text (enclosed with double quotes/ single quotes)
    - no limitation on individual text (but total document size should not exceed 16mb)
- booleans (true/false)
- numbers (32bit), numberLong(64 bit), numberDecimal (in shell, everything by default is 64bit float)
- objectId (special data to indentify a mongodb document uniquely)
- ISODate (store date objects) (new Date() in shell gets current date in date'T'time'Z' format)
- Timestamp (a long number based on current time in milliseconds) 
    (new Timestamp() in shell gets current date)
- embedded documents
- Arrays (of any of the above datatypes)

eg.
{name: "Dunzo", isStartup: true, employees: 33, funding:1234567890124567890, 
details:{ceo "unknown", launched:2020}, tags: [{title:"deivery"}, {title:"e-commerce"}],
foundingDate: new Date(), insertedAt: new Timestamp()
}
-- in above funding number will get truncated
-- so storing in string can b good idea

{a: NumberInt(1)} this will be lesser in size than {a: 1}
bcause former is Int32 and later is float64

- relation is mongodb
    -- to establish relation between two kind of documents
        eg. customer collection and address collection
        or customer collection and branches visited collection
    -- two ways to establish relation
        - embedded document (inside customer we can have address document or list of 
            address documents)
        - references
            - embedding doesn't work in all cases. It works for address bcz an address mostly will be
                associated with 1 or very few customers
            - but in case of branch visited, one brnch will be visited by many customers, so our
                data will contain lot of duplicates since all of data for a single branch will be stored
                with all customers visiting it
            - and if any of branch data changes, then we need to change it for all customers who ever 
                visited
            - so for references we only store id of branch and have a separate branch collection 

    eg. of references
    - db.patientdb.insertOne({name: "Max", age:29, diseaseSummaryId: "max-summary-1"})
    - db.diseaseSummaries.insertOne({_id: "max-summary-1", diseases: ["cold", "broken-leg"]})

    -- to combine both (2 steps)
    - db.patientdb.findOne({name: "Max"})
    - var dsid = db.patientdb.findOne({name: "Max"}).diseaseSummaryId
    - db.diseaseSummaries.findOne({_id = dsid})

    -- in this example, embedded document will be better (one to one relationship)
        -- if we're building a web app where we need to render both info
        -- but reference document will be better for a analytical use case 
            -- since we might need to analyse diseases alone, without info of patient

    -- relation for questions-answers thread (one to many relationship)
        -- one ques can have many answers, but one answer is for one ques only
        -- for such use case, embedded document is better
        -- in document which is on 'one' side (question) can embed 'many' side document (answers)

    -- relation for city-citizen (one to manny relationship)
        -- one city will have many citizens, but citizen can live in only one city
        -- if we use embedded doc, city document size might hit 16mb limit for very
            populated cities like Mumbai, delhi
        -- we get lot of useless citizen data if we just want to analyse city data
        -- for such use cases, references are better
        -- with each citizen, we can store the cityId, bcz if we store citizens with city
            the array will contains millions of elements
    
    -- relation for orders (customers-products) (many to many relationship)
        -- for such use cases, references is better
        -- we can have products collection and customers collection
        -- and then we can have order collection with customerid and productid
        -- or better we can store list of orders in customer collection 
        -- each element of order array can have an embedded document productid and quantity

        -- we can use embedded data with only one problem, lot of duplication
        -- so if we need to change a product, it has to be changed for all customer who brought it
        -- but if in the use case, even if product is updated, the record doesn't need to change
            bcz it maintains the snapshot of details while placing order, then embedded can be considered

    -- $lookup
        -- db.books.aggregate([{$lookup: {from: "author_collection", localField: "authors",
        foreignField: "_id", as:"creators"}}])
        -- used to join author collection with books collection
        -- will match "authors" column from books to "_id" column from authors
        -- it will have all existing keys along with new key "creators" having 
            author collection objects
        -- useful with references

assignment: user -> create/edit/delete/fetch one/ fetch many/ comments on : post
identify nd create mongodb collections and relations and way to store relation

### my solution 
collections -> users, posts, comments (correct based on video solution)
relations
user -> post (one to many)
post -> comments (one to many) (embedded) (correct based on video solution)
user -> comment (one to many) (reference) (correct based on video solution)

user: {userId, username, age, email} (correct based on video solution)
post: {postId, title, postContent, tags, 
        author(userId), comment : { userId, commentContent}} (correct based on video solution)

-- schema validation
db.createCollection("postsCollection", {
    validator: {
        $jsonSchema: {
            bsonType: "object",
            required: ["title", "text", "author", "comments"],
            properties: {
                title: {
                    bsonType: "string",
                    description: "must be a string and is requried"
                },
                text: {
                    bsonType: "string",
                    description: "must be a string and is requried"
                },
                author: {
                    bsonType: "objectId",
                    description: "id from user collection"
                },
                comments: {
                    bsonType: "array",
                    description: "array of comments on post, can be empty array",
                    item: {
                        bsonType: "object"
                        required: ["commentContent", "author"]
                        properties: {
                            commentContent: {
                                bsonType: "string"
                                description: "actual comment made my a user"
                            },
                            author: {
                                bsonType: "objectId",
                                description: "id from user collection"
                            }
                        }
                    }
                }
            }
        }
    }
})

-- using above validation if we pass a wrong value, insertion fails and document is not inserted
-- to modify validation of already crated collection
db.runCommand({collMod: "postsCollection", 
    validator: {
        $jsonSchema: {
            bsonType: "object",
            required: ["title", "text", "author", "comments"],
            properties: {
                title: {
                    bsonType: "string",
                    description: "must be a string and is requried"
                },
                text: {
                    bsonType: "string",
                    description: "must be a string and is requried"
                },
                author: {
                    bsonType: "objectId",
                    description: "id from user collection"
                },
                comments: {
                    bsonType: "array",
                    description: "array of comments on post, can be empty array",
                    item: {
                        bsonType: "object"
                        required: ["commentContent", "author"]
                        properties: {
                            commentContent: {
                                bsonType: "string"
                                description: "actual comment made my a user"
                            },
                            author: {
                                bsonType: "objectId",
                                description: "id from user collection"
                            }
                        }
                    }
                }
            }
        }
    },
    validationAction: "warn"
})

default validationAction is "error"

-- mongodb shell and server
-- dbpath is where actual mongodb data get written and --logpath is where logs get written

-- command to start mongo server with these configured
- mongod --dbpath 'path to folder to store db'  --logpath 'path to folder to store logs/logFileName.log'
-- for log folder we also need to provide a path to log file where to write it (not necessary to create
    it beforehand)

-- mongodb by default uses 'WiredTiger' as storage engine but we can change it

-- fork option (ony runs with mac and linux)
-- import to give logpath when running fork command
- mongod fork --logpath 'path to folder to store logs/logFileName.log'
-- now it will run as a service in background, and not block the cmd prompt
-- in windows, we have option to run it as a service (same as fork), which we 
    select while installation
-- command in windows: net start mongodb
-- to shutdown this service running in background:
-- in mongo shell type
- use admin
- db.shutdownServer()
-- this works in windows as well, alternatively in windows we can also use
- net stop mongodb

-- we can create a config file with all these options (so it is saved for future)
-- .cfg extension, look more in documentation
-- how to use it config file
- mongod -f 'path to config file'

-- create operation in details

- db.collection_name.insertOne({name: "sid", age: 25, hobbies: ["tv-series", "books"]})
-- this will automatically create an id with field name _id and type ObjectId

- db.collection_name.insertMany([{name: "ravi", age: 26, hobbies: ["tv-series", "movies"]},
 {name: "sowmya", age: 24])
 -- id will be created automatically for all the documents

-- insertOne takes a document as an argument (enclosed in {})
-- insertMany takes a list of documents as an argument (enclosed in [])
-- we can't simply pass ',' separated documents to insertMany as argument

- db.insert({name: "ritik", age:25})
- db.insert([{name: "Utkarsh", age:24}, {name: "Rana", age:26}])
-- insert can be used for one or many records

-- INSERT VS INSERTONE/INSERTMANY
-- insertOne and insertMany return back the id of newly created document which insert does not
-- when we create an application, we generally want tp return its id for further use
-- but with insert it is ineffecient bcz we have to wait for insertion to complete and the 
    query the entire collection again

-- create custom id instead of using auto-generated ids
-- we can explicitly mention '_id' field in each document
-- if we pass any duplicate id which is already present in the collection,
    then insert operation will fail and we'll get the error msg

-- ORDERED_INSERT: if we are bulkInserting (insertMany), then all documents before the duplicate
    was encountered will be added, but all other after the duplicate will fail 
    (even if they're correct) ...... This is the default behaviour of MongoDb, but can be changed
-- we can change the ORDERED_INSERT behaviour in insertMany
- db.collection_name.insertMany([{}, {}, {}], {ordered: false})
-- UNORDERED_INSERT after passing list of documents, we can pass a document of arguments
    in this case we have one argument, 'ordered' value of which by default is true,
    but we're setting it to false. So all the documents will be tried to insert, even if any
    of them fails. So we can have multiple failed documents as well as inserted documents

-- in any of the case, a ROLLBACK will not happends, i.e. whatever got inserted before failed document
    won't be implicitly removed from db on encountering a failed document

-- WRITECONCERN (writeConcern) => {w: 1, j:undefined}
-- w means write and number (it's value) is the number of instances of server we want
    it acknowledged
-- j stands for the journal (like a todo list)
-- we get acknowledged true msg bcz we have set w to 1

-- ATOMICITY
-- suppose there is a document with multiple fields and when writing it to the server
    write operation fails after writing some fields. If operation is not atomic, then we
    can have a partially written document in the database. But MongoDb gurantees ATOMICITY
    and hence a document would either be completly written or completly failed (done using rollback)
-- in case of bulk insert, atomicity is still at document level and not operation level
    
-- IMPORTING DATA
- mongoimport filepath_in_local_machine.json -d database_name -c collection_name --jsonArray --drop
-- if database or collection not present they'll created while running the command
-- json file can contain single document or array of multiple documents. By default command assumes
    it a single document. To specify otherwise, we use --jsonArray
-- by default if appends these documents, if collection already exists, but we can drop the 
    existing collection and create a new one with these documents using --drop

eg. mongoimport tv-shows.json -d siddhant -c tvshows --jsonArray --drop -u "admin" -p "pass" --authenticationDatabase admin

#### READ OPERATIONS IN MongoDb ####

# methods, filters and operators in mongodb
-- each command starts with db followed by collection_name then method name
-- so commands like insertOne, find, update are all different methods
-- methods can contain documents as arguments
-- a document inside method can simply be data or filter
eg. db.collection_name.find({age:32})
-- inside find method we passed a document which contains a filter that age should be
    equal to 32

-- there can be more complex filters:
eg. db.collection_name.find( {age: {$gt: 30}} )
-- here $gt is an operator (greater than) [it is a range operator]
-- all operators starts with a '$'

-- type of operators
    -- query (eg. $eq) & projection (eg. $, $elemmatch, $meta, $slice) operators
    -- update (eg. $inc)
    -- query modifiers (depricated)
    -- aggregations


# query selectors
to check all operators, go to mongodb docs -> mngodb server -> reference -> operators

# COMPARISON OPERATORS 
$eq -> db.tvshows.find({ runtime: {$eq: 60} })
    -- eq is the defaulte comparison oprator and hence we can simply write: 
        db.tvshows.find({runtime: 60})
$gt -> db.tvshows.find({ runtime: {$gt: 60} })
$gte -> db.tvshows.find({ runtime: {$gte: 60} })
$ne (not equal) -> db.tvshows.find({ runtime: {$ne: 60} })
$lt (less than) -> db.tvshows.find({ runtime: {$lt: 60} })
$lte (less than equal) -> db.tvshows.find({ runtime: {$lte: 60} })
$in (value in mentioned array) -> db.tvshows.find({runtime: {$in: [30,42,73]}})
$nin (not in (opposite of $in)) -> db.tvshows.find({runtime: {$nin: [30,42,73]}})

-- can be used with embedded field as well
-- we need to mention field in double quotes (otherwise it is considered invalid)
db.tvshows.find({"rating.average": {$gt: 7}}).pretty()

-- using operator with Arrays
-- find document where genre is "Drama" (case sensitive)
db.tvshows.find({genres: "Drama"}).pretty()
-- finding documents with exactly matching array (only contains drama)
db.tvshows.find({genres: ["Drama"]}).pretty()

# PRETTY and COUNT
.pretty() -> adding this at end prints it formatted with nested look
.count() -> adding this at end prints returns the no. of docs to be rturned by the query preceding it


# LOGICAL OPERATORS
$or -> eg. finding all docs with rating less than 5 or more than 9.3
    - individual queries
    db.tvshows.find("ratings.average": {$lt: 5})
    db.tvshows.find("ratings.average": {$gt: 5})
    - combining both queries
    db.tvshows.find({$or: [{"rating.average": {$lt: 5}}, {"rating.average": {$gt: 9.3}}]}).count()

$nor -> neither of the conditions are met
    db.tvshows.find({$nor: [{"rating.average": {$lt: 5}}, {"rating.average": {$gt: 9.3}}]}).count()

$and -> db.tvshows.find({$and: [{"rating.average": {$gt: 9}}, {genres: "Drama"}]})
    Note: genres containing drama and not genres exactly equal to drama
    -- $and is the default logical operator so we can simply write it as:
    db.tvshows.find({"rating.average": {$gt: 9}, genres: "Drama"})
    -- this default way will not work when we have multiple condition on same column 
    -- bcz some drivers (eg. javascript) don't allow duplicate keys in their dictionary
    -- and also last condition on same key replaces all the previous ones (in all drivers)
    eg. db.tvshows.find({genres: "Drama", genres: "Horror"})
    is equivalent to db.tvshows.find({genres: "Horror"})

$not -> db.tvshows.find({runtime: {$not: {$eq: 60}}}).count()
    -- easier way to write this would be simply db.tvshows.find({runtime: {$ne: 60}}).count()

# ELEMENT OPERATORS
$exists -> find all documents where the field exists/doesn't exists 
    db.collection_name.find({field_name: {$exists: true}})
    -- can be combined with other filters as well
    db.collection_name.find({field_name: {$exists: true, $gt: 30}})
    -- if any document has the field but value is null, then also it will show in (exists: true)
    -- to eliminate null values add additional filter
    db.collection_name.find({field_name: {$exists: true, $ne: null}})

$type -> find documents wher field contains value of specific data type
    db.collection_name.find({field_name: {$type: "number"}})
    -- we can give array of datatypes, any one of which if satisfies, the doc wil be displayed
    db.collection_name.find({field_name: {$type: ["doble", "string"]}})

# EVALUATION OPERATORS
$regex -> to specify a regular expression (not very efficient)
    db.tvshows.find({name: {$regex: /Game/}})

$expr -> compare two fields of a document
    eg. a document: {volume:200, target: 120}, {volume: 120, target:300}
    db.collection_name.find({$expr: {$gt: ["$volume", "$target"]}})
    -- only certain operators work with expr (can be looked in documentation)
    -- column names should be enclosed in double quotes and start with $ sign
    -- without $ sign they are just treated has hardcoded strings

    more complex expression (to find all documents where volume>190 and volume-target>=10):
    db.collection_name.find({$expr: {$gt: [{$cond: {if: {$gte: ["$volume", 190], 
    then: {$subtract: ["$volume", 10]}},
    else: "$volume"
    }}, "$target"]}})

# QUERYING ARRAYS
-- query arrays with embedded documents
    eg. document: {name:"xyz", age: 32, hobbies: [{title: "reading books", rating: 5}, 
                    {title: "video games", rating: 4}]}

    -- finding hobby "reading books"
    db.collection_name.find({hobbies: "reading books"}) -- wrong (will find exactly this value in array)
    db.collection_name.find({hobbies: {title:"reading books"}}) -- wrong (will find exactly this document in array)
    db.collection_name.find({"hobbies.title": "reading books"}) --right (same like a single embedded document)

$size -> find size of array
    -- with size, we can only use exact matches and not greater than, less than etc.
    db.collection_name.find({hobbies: {$size: 3}})

$all -> array should contains all the specified elements in any order (other elements can also be present)
    db.collection_name.find({arrayColumn: ["elem1", "elem2"]})
    -- above exact match won't work, bcz if a document has same elements in different order they
        will not be returned
    db.collection_name.find({arrayColumn: {$all: ["elem1", "elem2"]}})

$elemMatch -> 
    eg document: {name: "xyz", hobbies: [{title: "sports", "freq":3},{title: "cooking", freq:6}]}
    -- find all elements where freq of sports is greater than 3
    db.collection_name.find($and: [{"$hobbies.title": "sports"}, {"$hobbies.freq": {$gt: 3}}])
    -- above document should not be returned but it will be, bcz two and conditions are connected
        and hence if it finds any array element with freq>3 it will take it as true
    db.collection_name.find({hobbies: {$elemMatch: {
        {title: "sports", freq: {$gt: 3}}
    }}})

# CURSORS
find() method gives us a cursor (unlike findOne())
cursor is basically a pointer which gives batches of data (to reduce too much of data transfers)
for shell, default batch size is 20

cursor has sevral methods like
cursor = db.collection_name.find()
cursor.next() -> gives next document
cursor.hasNext() (boolean value)
forEach -> cursor.forEach(doc => {printjson(doc)}) (javascript way)

sorting cursor -> db.collection_name.find().sort({"rating.average": 1, runtime: -1})
    - 1 is for ascending and -1 for descending

skipping and limiting cursor results (used with pagination in apps)
.skip() -> skip n number of documents from starting
    db.collection_name.find().sort({"rating.average": 1, runtime: -1}).skip(10)
.limit() -> limit no of documents cursor retrieves at one time
    db.collection_name.find().sort({"rating.average": 1, runtime: -1}).limit(10)
-- we can write sort, limit, skip in any order but mongodb always does it in same 
    order (sort -> skip -> limit)

# PROJECTION -> control what data is returned
db.collection_name.find({}, {name: 1, genres:1, runtime: 1, rating:1, "schedule.time":1,  _id:0})
-- first argument is any filter, if there are none we can simply pass an empty doc
-- second argument is the projection
-- if we mention 1 it is included, if we mention 0 it is excluded
-- by default every field has a 0, except objectId which has a 1
-- to remove objectId while displaying we need to explicitly mention it with a 0

-- projection on arrays
    db.collection_name.find({genres: {$all: ["Drama", "Horror"]}}, {"genres.$": 1})
    -- this will give only first macth, only o/p horror for genres
    -- bcz after matching horro it confirms than document has both values and then returns first match only

    -- we can make it more complex, since projection just excepts a boolean value (1/0)
        we can create that with our own conditions
    db.tvshows.find({genres: "Drama"}, {genres: {$elemMatch: {$eq: "Horror"}}})

$slice -> 
    -- get first n elements of an array
    db.tvshows.find({}, {name:1, genres: {$slice: 2}})

    -- if we give in array it will skip first m values and take next n values 
    -- in below case it will skip first 2 values and take next 4 values
    db.tvshows.find({}, {name:1, genres: {$slice: [2,4]}})

#### UPDATE OPERATIONS IN MongoDb ####
update is a two step process:
1) identify document that needs to be changed 2) make the changes

updateOne -> takes first matching document and makes the update even if there are multiple documents 
    matching the conditions
updateMany -> will update all documents matching the given conditions

$set -> adds new fields to the matching documents, doesn't change the ObjectId
    db.collection_name.updateOne({_id:1}, {$set: {hobbies: ["elem1", "elem2"]}})
    -- here these elements won't be added to the existing array, but simply overwrite previous 
        array with this new one
    db.collection_name.updateMany({"hobbies.title": "sports"}, {$set: {isSporty: true}})

    -- we can aslo set multiple fields
    db.collection_name.updateOne({_id:1}, {$set: {age:40, phonenum: 8004455667}})

# Incrementing and decrementing values
$inc -> db.collection_name.updateOne({_id:1}, {$inc: {age: 2}})
to decrement ->db.collection_name.updateOne({_id:1}, {$inc: {age: -1}})

multiple update operators can be given in same statement
db.collection_name.updateOne({_id:1}, {$inc: {age: 2}, $set: {isSporty: false}})

-- conflicting updates will throw error
eg. db.collection_name.updateOne({_id:1}, {$inc: {age: 2}, $set: {age: 30}})
-- here we are doing 2 different operations to same field age and hence it is a conflict

# $min, $max, $mul
$min -> it will change the value, if new value is less than what is currently stored 
    db.collection_name.updateOne({_id:1}, {$min: {age:35}}) 
    -- so if current age is say 38 then it will make it 35, but if it 34 it will stay the same

$max -> same logic as min
$mul -> multiply
    -- if we want to give 10%hike .. we can simply multiply current salary with 1.1
     db.collection_name.updateOne({_id:1}, {$mul: {salary: 1.1}})

# removing a field if condition matches
-- one solution is given below, but setting to null is not same as removing
    db.collection_name.updateOne({isSporty:true}, {$set: {phone:null}})
-- correct method given below
$unset ->opposite of $set, $set is for adding new field/updating existing field
    while $unset is to delete an existing field
db.collection_name.updateOne({isSporty:true}, {$unset: {phone:""}})

$rename -> to rename a column (below doing it across all documents)
    -- if a document doesn't contain the old field, it will not have the 
        new field also, since it is just a rename
db.collection_name.updateOne({}, {$rename: {salary: grossSalary}})

# $upsert (update/insert)
-- if we run below query, the document will be lost since this document doesn't exist
    so nothing to update, and if we tried to insert it as new document and it was already
    present then it will throw error opr create duplicate (for scenarios where we don't 
    know whether doc exists or not)
db.collection_name.updateOne({name: "newGuy"}, {$set: {age:35, hobbies: [h1,h2], isSporty:true}}, {upsert: true})
-- upsert is the third argument, default value is false

# updating matched array elements
-- update document where hobbies have sports with frequency greater than or equal to 3
    db.collection_name.updateMany({hobbies: {$elemMatch: {title: "sports", freq: {$gte: 3}}}}, 
    {$set: {"hobbies.$.highFreq": true}})
-- we don't want to change complete hobbies element, which is part of root document
-- but some elements inside hobbies, so we use '.' and enclose in quotes
-- also not hobbies elements but only which matched the condition so we use '.$' to
    refer to each matched element, inside which we are adding a new field
-- in this for each document, only first element of hobbies array for each document will be
    updated. Since in this case each document will have only one element with one title so
    it should not be a problem

# update all elements of array for matching parent document
-- update documents where freq is greater than 2 
    db.collection_name.updateMany({totalAge: {$gt: 30}}, {$inc: {"hobbies.$[].frequency": -1}})
-- arrayName.$ meant for matching condition documents first array item
-- arrayName.$[] means for matching condition documents all array item
-- we can use any of $set, $inc etc.

# update some of elements of array for matching condition
    db.collection_name.updateMany({"hobbies.freq": {$gt:2}}, {$set : {"hobbies.$[el].goodFreq": true}},
    {arrayFilters: [{"el.freq" : {$gt:2}}]})
-- arrayFilters is third argument, in this only we give args like upsert and writeConcern
-- el is an identifier given by us, we can choose any name
-- first filter need not be related to array, it can be anything, it is simply to shortlist the 
    parent document whose array we'll be checking
-- in arrayFilters we give the condition to filter elements of array which need to be updated,
    in this case it matches the first arg but not always necessary
-- same variable should be mentioned in 2nd arg also

# Adding elements to arrays
-- adding one document
$push -> db.updateMany({age: {$gt: 30}}, {$push: {hobbies: {title: "praying", freq:5}}})

-- adding multiple documents
$each -> db.updateMany({age: {$gt: 30}}, {$push: {hobbies: {$each: [
    {title: "praying", freq:5},
    {title: "meetings", freq:7}
], $sort: {freq: -1}}}})
    -- $sort is a sibling of each (is added in same document)
    -- it sorts before insert it in the main document, $slice can also be used
    -- entire array is sorted (including existing elements)

$addToSet -> similar to push
    -- but it takes only one document at a time, so $each won't work with it
    -- it doesn't allow to enter a duplicate to the array
    -- it won't throw error, but nothing will be inserted
    

# removing elements to arrays
$pull -> db.collection_name.updateMany({age: {$gt: 30}}, {$pull: {hobbies: {title: "praying"}}})
    -- pull takes a condition, we can use other operators also like $gt etc.
$pop -> takes out only the last element
        db.collection_name.updateMany({age: {$gt: 30}}, {$pop: {hobbies: 1}})
    -- 1 means last element and -1 means first element

#### DELETE OPERATIONS IN MongoDb ####

db.collection_name.deleteOne({name: "personName"})
-- the argument can take anything which we pass in a find or update op to filter documents
-- only delete first record that macthes

-- delete all people who have sports as hobby and age field doesn;t exists
db.collection_name.deleteMany({$and: [{totalAge: {$exists: false}}, {"hobbies.title": "sports"}]})

# delete all entries in a collection
db.collection_name.deleteMany({})
OR
db.collection_name.drop()

# delete a database
db.dropDatabase()

#### INDEXES IN MongoDb ####
-- index can speed up find, update and delete queries
-- suppose a collection has documents with each of them having a field 'name'.
    if we want to search document with name "xyz" in the collection and there's no index set then 
    mongodb will do a collscan (collection scan) -- default approach
-- collscan -> to look through every single document and check if name present or not (for large 
    collections this can make the query very slow)
-- ixscan -> first searches the seller index and then get all the documents

-- index -> is a ordered list of field on which we have created the index (not the entire document but
    just the field) -> which also contains list of pointers to all document containing this value for 
    the given field

-- we shouldn't use too many indexes also. Bcz each index comes with a price. The find, update and 
    delete queries get speed up but insert becomes slow 

-- importing person.json file
    mongoimport persons.json -d siddhant -c contacts --jsonArray --drop -u "admin" -p "pass" --authenticationDatabase admin

-- mongodb has a explain() method which can be used with find, update and delete (not insert)
    -- it tell what mongodb does in the background
    -- it has a key called "winningPlan" (with nested key called stage) 
        which tells what search startegy was used (eg. COLLSCAN)
    db.collection_name.explain().find({"dob.age": {$gt: 60}})

db.collection_name.explain("executionStats").find({"dob.age": {$gt: 60}})
-- above command will give details like executionTimeMillis (milliseconds), totalDocsExamined

# create index
- db.collection_name.createIndex({"dob.age": 1}) #1 is for ascending, -1 for descending

-- now if we run the query: db.collection_name.explain("executionStats").find({"dob.age": {$gt: 60}})
    -- the executionTimeMillis will reduce
    -- and totalDocsExamined will be equal to only number of documents satisfying the criteria
        and not all the documents in collection(which was the case in COLLSCAN)
    -- the stages will not be COLLSCAN but instead they will be IXSCAN (input stage) -> FETCH (get docs)

# drop index
- db.collection_name.dropIndex({"dob.age":1})

-- if we repeat the above query with age>20 (age>20 would mean all the documents) 
    instead of age>60 with two scenarios
    -- indexing and -- COLLSCAN
    -- collscan will be faster bcz indexing has an additional step to look through keys and
        then read the documents
    -- so if index covers large portion of documents then it should not be taken
    -- for eg. city can be a good index but country would be ineffecient (if 2-3 country data is there)

# create compound indexes
- db.collection_name.createIndex({"dob.age":1, gender:1})

-- will not create two indexed but one with two fields
-- order of writing matters
-- if we do db.collection_name.explain().find({"dob.age": 35, "gender":"male"})
    or if we do db.collection_name.explain().find({"dob.age": 35}) 
    it will do index scan with this same compund index
-- but if we do db.collection_name.explain().find({"gender":"male"}) then it will do a COLLSCAN
    since dob is the first index

#sorting using indexing
-- indexing not only helps in finding but also in sorting
- db.contacts.explain().find("dob.age":35).sort({gender: 1})

# to see all the indexes for a collection
- db.collection_name.getIndexes()
-- id is a default index

# configuring index (setting field to be unique)
- db.collection_name.createIndex({email: 1}, {unique: true})
-- will give error if this field has duplicate values already
-- only one null value can be allowed for this field, after that it'll give duplicate error

we can solve null value problem using partialFilterExpression (explained below)
- db.collection_name.createIndex({email: 1}, {unique: true, 
                                            partialFilterExpression: {email: {$exists: true}}})

# partial filters
- db.collection_name.createIndex({"dob.age": 1}, {partialFilterExpression: {gender: "male"}})
-- if for our use case, we want to index only a part of the documents which we'll query more
    often to save memory and improve performance we can use partial filters
-- in above query index on age will be created but only for docs where gender is male
-- so if we simply do a find on age>60, COLLSCAN will be done since it is on both genders
-- if we add filter for gender=male also then it will do IXSCAN

-- diff b/w compund and partial (since in both we have to mention all conditions)
-- partial minimizes the keys and also write queries speed up since if it's a female then
    no worrying about indexing

# Time-to-live (TTL) index
-- best suited for self-destroying data (like sessions of users etc.)
- db.collection_name.createIndex({timeSensitiveCol:1}, {expireAfterSeconds:10})
-- all existing and new documents with index which has passed 10 seconds will be deleted from the db
-- evaluation of existing documents for expiration is considered after a new element is added and
    not immediately after adding the index
-- only works on date objects

# multi key indexes
-- we can use columns which contain array values as index also
--  these will be multikey index, since not entire array is the index, but each element of
    array becomes a index
-- that means if there are 100 docs then there won't be 100 keys but more than that depending
    on number of unique elements in all arrays
-- we can also use an array of documents also as index
-- but if we search based on the field of document which is used as index, then
    mongodb will do a COLLSCAN
-- it will do IXSCAN only when we search the complete array document

eg. doc = {name: "abc", hobbies: ["reading", "swimming"], address: [
    {"street": "street1", city: "Jhansi"},
    {street: "street2", city: "jaipur"}
]}

now if we use address as the index:
-- if we do db.collection_name.explain("executionStats").find({"address.street": street1})
    it will do a COLLSCAN
-- but if we do db.collection_name.explain("executionStats").find({"address": {"street": "street1", city: "Jhansi"}})
    it will do a IXSCAN

--if we want first case to be IXSCAN as well, then we can add address.street also as index.
    But this can have big performance issues


- we cannot have two arrays in a compund index (bcz it will have to do a cartesian product
    for all combinations) which can lead to performance issues







    https://www.google.com/search?q=practice+mongodb&rlz=1C1CHZL_enIN823IN823&oq=practice+mon&aqs=chrome.1.69i57j0i512l2j0i10i131i433j0i512j0i10i131i433j0i10j0i512j0i10j0i10i131i433.3687j0j7&sourceid=chrome&ie=UTF-8
    https://www.w3resource.com/mongodb-exercises/
    https://www.google.com/search?q=hape+of+Water&rlz=1C1CHZL_enIN823IN823&sourceid=chrome&ie=UTF-8
    https://www.google.com/search?q=Under+The+Skin&rlz=1C1CHZL_enIN823IN823&sourceid=chrome&ie=UTF-8